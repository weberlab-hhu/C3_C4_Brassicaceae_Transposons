{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82f050c6-7434-45c3-b237-6cbcde6b754f",
   "metadata": {},
   "source": [
    "### This script is to correlate TEs to genes based on annotation comparison. It needs to be run for each species seperately and will generate one .csv file per species, containing the gene and associated TE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3138262-9fb4-44f8-9117-9cd5cd93c8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None  # default='warn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c65150",
   "metadata": {},
   "outputs": [],
   "source": [
    "TE_gff_raw  = pd.read_csv('<EDTA_results>/<species>.fasta.mod.EDTA.intact.gff3', sep = '\\t', \n",
    "                      header = None, index_col=0)\n",
    "TE_gff = TE_gff_raw[~TE_gff_raw[2].isin(['long_terminal_repeat', 'repeat_region', 'target_site_duplication'])] #delete all rows containing TIR TSD and repeat info\n",
    "\n",
    "genome_gff = pd.read_csv('<genome_annotation>/<species>.gff3', sep = '\\t', engine = 'python', header = None, index_col = 0)\n",
    "\n",
    "#create path for csv output file. The script will generate a .csv file with each TE associated to a gene and the type of association (upstream, downstream,---)\n",
    "csv = '<species_output_file>.csv'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "121d3093",
   "metadata": {},
   "source": [
    "## correlate TEs to genes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b370e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this version only includes genes as features, leaves UTR annotations etc\n",
    "genome_gff = genome_gff[genome_gff[2] == 'gene']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f374edd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "contigs_with_TE = list((set(genome_gff.index)) & (set(TE_gff.index))) #only use contigs where TEs and genes are present, leave the rest\n",
    "list_of_ignored_contigs = []\n",
    "subset_genome = genome_gff\n",
    "subset_genome['location'] = np.nan #to have 'location' in header\n",
    "subset_TE = TE_gff\n",
    "subset_genome.iloc[[1]][1:].to_csv(csv, header = True) #make blank csv with header\n",
    "\n",
    "for c in range(len(contigs_with_TE)): \n",
    "    location = []\n",
    "    df_hit = pd.DataFrame()\n",
    "    df_hit  = subset_genome.iloc[[0]] #placeholder, delete later\n",
    "    if contigs_with_TE[c] in TE_gff.index and contigs_with_TE[c] in genome_gff.index:\n",
    "        subset_TE = TE_gff.loc[[contigs_with_TE[c]]]\n",
    "        subset_genome = genome_gff.loc[[contigs_with_TE[c]]]\n",
    "    else:\n",
    "        list_of_ignored_contigs.append(contigs_with_TE[c]) #for file with list of ignored contigs\n",
    "\n",
    "    for i in range(len(subset_TE)):\n",
    "        for m in range(len(subset_genome)):\n",
    "            if subset_genome.iloc[m][3] < subset_TE.iloc[i][3] < subset_genome.iloc[m][4]: #if TE starts in feature\n",
    "                df_hit = df_hit.append(subset_genome.iloc[[m]])\n",
    "                location.append('start')\n",
    "            if subset_genome.iloc[m][3] < subset_TE.iloc[i][4] < subset_genome.iloc[m][4]: #if TE ends in feature\n",
    "                df_hit = df_hit.append(subset_genome.iloc[[m]])\n",
    "                location.append('end')\n",
    "            if subset_genome.iloc[m][3] < subset_TE.iloc[i][3] < subset_TE.iloc[i][4] < subset_genome.iloc[m][4]: #if TE within feature\n",
    "                df_hit = df_hit.append(subset_genome.iloc[[m]])\n",
    "                location.append('within')\n",
    "            if subset_TE.iloc[i][3] < subset_genome.iloc[m][3] < subset_genome.iloc[m][4] < subset_TE.iloc[i][4] : #if TE over feature\n",
    "                df_hit = df_hit.append(subset_genome.iloc[[m]])\n",
    "                location.append('over')  #weird look to keep over when removing duplicates\n",
    "                \n",
    "                            \n",
    "            if subset_genome.iloc[m][3]-3000 < subset_TE.iloc[i][3] < subset_genome.iloc[m][3] : #if TE end up to 3000 bp upstream of gene start\n",
    "                df_hit = df_hit.append(subset_genome.iloc[[m]])\n",
    "                if subset_genome.iloc[m][6] == '-': #check for direction of gene\n",
    "                    location.append('downstream') \n",
    "                else:\n",
    "                    location.append('upstream') \n",
    "                \n",
    "            if subset_genome.iloc[m][3]-3000 < subset_TE.iloc[i][4] < subset_genome.iloc[m][3] : #if TE start up to 3000 bp upstream of gene start\n",
    "                df_hit = df_hit.append(subset_genome.iloc[[m]])\n",
    "                if subset_genome.iloc[m][6] == '-': #check for direction of gene\n",
    "                    location.append('downstream') \n",
    "                else:\n",
    "                    location.append('upstream') \n",
    "                \n",
    "            if subset_genome.iloc[m][4] < subset_TE.iloc[i][3] < subset_genome.iloc[m][4]+3000 : #if TE start up to 3000 bp downstream of gene end\n",
    "                df_hit = df_hit.append(subset_genome.iloc[[m]])\n",
    "                if subset_genome.iloc[m][6] == '+': #check for direction of gene\n",
    "                    location.append('downstream') \n",
    "                else:\n",
    "                    location.append('upstream') \n",
    "            if subset_genome.iloc[m][4] < subset_TE.iloc[i][4] < subset_genome.iloc[m][4]+3000 : #if TE end up to 3000 bp downstream of gene end\n",
    "                df_hit = df_hit.append(subset_genome.iloc[[m]])\n",
    "                if subset_genome.iloc[m][6] == '+': #check for direction of gene\n",
    "                    location.append('downstream') \n",
    "                else:\n",
    "                    location.append('upstream') \n",
    "                \n",
    "                \n",
    "    df_hit_1 = df_hit[1:]\n",
    "    df_hit_1['location'] = location #add location to df\n",
    "    df_hit_1.to_csv(csv, mode = 'a', header = False) #open csv and append \n",
    "    \n",
    "#now deduplicate and sort everything\n",
    "df_hit_1 = pd.read_csv(csv, sep = ',', engine = 'python', header = [0], index_col = 0)\n",
    "df_hit_1 = df_hit_1[df_hit_1['2'].isin(['gene'])] #keep only ORFs\n",
    "df_hit_1 = df_hit_1.drop_duplicates(subset='8', keep ='first') \n",
    "df_hit_1 = df_hit_1.sort_values(by=['0'], ascending=False) #sort to keep \"over\"in duplicates\n",
    "df_hit_1.to_csv(csv, mode = 'w', header = True) #open csv and append "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f726bb4",
   "metadata": {},
   "source": [
    "### now add Orthogroup of analyzed species"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d2569d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8312ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df = pd.read_csv(csv)\n",
    "\n",
    "df ['8'] = [i.split('.')[0][3:] for i in df['8']] #reformat ID compatible with Orthofinder Results\n",
    "\n",
    "ogs_list = []\n",
    "orthogroups = pd.read_csv('<EDTA_results></Orthogroups/Orthogroups.tsv', sep = '\\t', dtype={'user_id': int})\n",
    "for i in df['8']: #compare our csv file with tsv file from OrthoFinder\n",
    "    try:\n",
    "        A = (orthogroups['<species>'])\n",
    "        match = A[A.str.contains(i, na = False)] #does the csv column contain an OG from Orthofinder?\n",
    "        ogs_list.append(orthogroups.loc[int(str(match).split(' ')[0])]['Orthogroup']) \n",
    "    except ValueError:\n",
    "        ogs_list.append('no_OG')\n",
    "        continue\n",
    "df ['orthogroups'] = ogs_list\n",
    "df.to_csv(csv, mode = 'w', header = True) #open csv and append "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c011db22",
   "metadata": {},
   "source": [
    "### extract CDS files from found genes associated to TEs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "778a2db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now get seqs from the CDS found:\n",
    "#open CDS file:\n",
    "\n",
    "#make cds dict\n",
    "cds = cds_raw.split('>')\n",
    "cds_dic = {}\n",
    "for i in range(len(cds))[1:]:\n",
    "    cds_dic[cds[i].split('\\n')[0].split(' ')[0]] = ''.join(cds[i].split('\\n')[1:]).upper()\n",
    "    \n",
    "#get ID's from genes spanned by TE\n",
    "ids_spanned = [i.split('=')[1] for i in df['ID']] \n",
    "#now write CDS of these genes to fasta file\n",
    "write = []\n",
    "for i in ids_spanned:\n",
    "    write.append('>')\n",
    "    write.append(i)\n",
    "    write.append('\\n')\n",
    "    write.append(cds_dic[i])\n",
    "    write.append('\\n')\n",
    "write = ''.join(write)\n",
    "text_file = open('<output_cds>.fasta', 'w')\n",
    "text_file.write(write)\n",
    "text_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd87d397",
   "metadata": {},
   "source": [
    "### the above file can now be uploaded to Mercator "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f46930e-efab-4ebd-9173-33a48bc63c5d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
